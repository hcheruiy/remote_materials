{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to Scikit-learn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPUJ8GtK79t0LdCbX7Nb9nU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hcheruiy/remote_materials/blob/master/Introduction_to_Scikit_learn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji9VyHKK1hHo",
        "colab_type": "text"
      },
      "source": [
        "`scikit-learn` is a Python library that provides a standard interface for implementing machine learning algorithms. It includes other ancillary functions that are integral to the machine learning pipeline such as data preprocessing steps, data resampling\n",
        "techniques, evaluation parameters, and search interfaces for tuning/optimizing an algorithmâ€™s performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1dZVgwB1-59",
        "colab_type": "text"
      },
      "source": [
        "## Loading Sample Datasets from Scikit-learn\n",
        "`scikit-learn` comes with a set of small standard datasets for quickly testing and\n",
        "prototyping machine learning models. They are small and well curated, so they do not represent real-world scenarios. FIve popular ones are:\n",
        " - Boston house-prices dataset\n",
        " - Diabetes dataset\n",
        " - Iris dataset\n",
        " - Wisconsin breast cancer dataset\n",
        " - Wine dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keHRHv322jsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load library\n",
        "from sklearn import datasets\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsC51lJ32wnW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afd9bbb4-d42f-4582-e5a5-2426d276b658"
      },
      "source": [
        "# load iris\n",
        "iris = datasets.load_iris()\n",
        "iris.data.shape"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_Ky-ooE2yCG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "95dab52f-1442-4168-e243-bf3b2dd7c565"
      },
      "source": [
        "iris.feature_names"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sepal length (cm)',\n",
              " 'sepal width (cm)',\n",
              " 'petal length (cm)',\n",
              " 'petal width (cm)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSQyAshG2_Jr",
        "colab_type": "text"
      },
      "source": [
        "## Splitting the Data into Training and Test Sets\n",
        "\n",
        "A core practice in machine learning is to split the dataset into different partitions for training and testing. `scikit-learn` has a convenient method to assist in that process\n",
        "called `train_test_split(X, y, test_size=0.25)`, where $X$ is the design matrix or dataset of\n",
        "predictors and $y$ is the target variable. The split size is controlled using the attribute `test_size`. By default, `test_size` is set to 25% of the dataset size. It is standard practice to shuffle the dataset before splitting by setting the attribute `shuffle=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu4Ql7Kv20y-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c471160-756d-4759-9dc6-7a1870537a22"
      },
      "source": [
        "# import module\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,\n",
        "                                                    shuffle = True)\n",
        "X_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(112, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGrW2WGD3qhU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c338463-e207-4d4c-9dc8-5cf193eaabbf"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okhoUdMX3sPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0a770b5-d8f3-4cf6-c438-670b3e2e05b1"
      },
      "source": [
        "print(y_train.shape, y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(112,) (38,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ct6Xh0X32gk",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the Data for Model Fitting\n",
        "\n",
        "Before a dataset is trained or fitted with a machine learning model, it necessarily undergoes some vital transformations. These transformations have a huge effect on the\n",
        "performance of the learning model. Transformations in `scikit-learn` have a `fit()` and `transform()` method, or a `fit_transform()` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3ruVZnV4S8O",
        "colab_type": "text"
      },
      "source": [
        "Depending on the use case, the `fit()` method can be used to learn the parameters of the dataset, while the `transform()` method applies the data transform based on the learned parameters to the same dataset and also to the test or validation datasets\n",
        "before modeling. Also, the `fit_transform()` method can be used to learn and apply the transformation to the same dataset in a one-off fashion. Data transformation packages are found in the `sklearn.preprocessing` package. They include:\n",
        "- data scaling\n",
        "- standardization\n",
        "- normalization\n",
        "- binarization\n",
        "- encoding categorical variables\n",
        "- imputing missing data\n",
        "- generating higher order polynomial features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyx3k1im4wCS",
        "colab_type": "text"
      },
      "source": [
        "### Data Rescaling\n",
        "\n",
        "Different scale for units of observations in the same dataset can have an adverse effect for certain machine learning models, especially\n",
        "when minimizing the cost function of the algorithm because it shrinks the function\n",
        "space and makes it difficult for an optimization algorithm like gradient descent to find the global minimum.\n",
        "When performing data rescaling, usually the attributes are rescaled with the range of 0 and 1. Data rescaling is implemented in `scikit-learn` using the `MinMaxScaler` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWbzMw5u3wS8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4510a147-fe9b-4c8d-e247-b469d933d098"
      },
      "source": [
        "#import packages\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# load dataset\n",
        "data = datasets.load_iris()\n",
        "\n",
        "# separate features and target\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# first 5 rows of Xbefore rescaling\n",
        "X[0:5, ]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ux-HUyc5g-P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a234a6bc-2ed8-4ee2-8321-ecc27b495e1e"
      },
      "source": [
        "# rescale X\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "rescaled_X = scaler.fit_transform(X)\n",
        "\n",
        "# first 5 rows of X after rescaling\n",
        "rescaled_X[0:5, ]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.22222222, 0.625     , 0.06779661, 0.04166667],\n",
              "       [0.16666667, 0.41666667, 0.06779661, 0.04166667],\n",
              "       [0.11111111, 0.5       , 0.05084746, 0.04166667],\n",
              "       [0.08333333, 0.45833333, 0.08474576, 0.04166667],\n",
              "       [0.19444444, 0.66666667, 0.06779661, 0.04166667]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6KyVJP954Vw",
        "colab_type": "text"
      },
      "source": [
        "### Standardization\n",
        "\n",
        "Linear machine learning algorithms such as linear regression and logistic regression make an assumption that the observations of the dataset are normally distributed with a mean of 0 and standard deviation of 1. However, this is often not the case with real-world datasets as features are often skewed with differing means and standard\n",
        "deviations.\n",
        "\n",
        "Applying the technique of standardization to the datasets transforms the features into a standard Gaussian (or normal) distribution with a mean of 0 and standard deviation of 1. `scikit-learn` implements data standardization in the `StandardScaler`\n",
        "module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwmiZW0I52PO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c3347a31-33b1-4176-ab25-be22ce46fbe0"
      },
      "source": [
        "# import packages\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# load dataset\n",
        "data = datasets.load_iris()\n",
        "\n",
        "# separate features and target\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# first 5 rows of X before standardization\n",
        "X[0:5, ]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIMGmi0Y6nIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bcbec83b-8de5-4693-f6d6-3bba8e7cb018"
      },
      "source": [
        "# standardize X\n",
        "scaler = StandardScaler().fit(X)\n",
        "standardize_X = scaler.transform(X)\n",
        "\n",
        "# first 5 rows of X after standardization\n",
        "standardize_X[0:5, ]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.90068117,  1.01900435, -1.34022653, -1.3154443 ],\n",
              "       [-1.14301691, -0.13197948, -1.34022653, -1.3154443 ],\n",
              "       [-1.38535265,  0.32841405, -1.39706395, -1.3154443 ],\n",
              "       [-1.50652052,  0.09821729, -1.2833891 , -1.3154443 ],\n",
              "       [-1.02184904,  1.24920112, -1.34022653, -1.3154443 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE84QJEx69jR",
        "colab_type": "text"
      },
      "source": [
        "### Normalization\n",
        "\n",
        "Data normalization involves transforming the observations in the dataset so that it has a unit norm or has magnitude or length of 1. The length of a vector is the square root of the sum of squares of the vector elements. A unit vector (or unit norm) is obtained by dividing the vector by its length. Normalizing the dataset is particularly useful in\n",
        "scenarios where the dataset is sparse (i.e., a large number of observations are zeros) and also has differing scales. Normalization in `scikit-learn` is implemented in the `Normalizer`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBKb8GWq643n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e135a6d6-7c88-4df6-a747-32d0b6652ccf"
      },
      "source": [
        "# import packages\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# load dataset\n",
        "data = datasets.load_iris()\n",
        "\n",
        "# separate features and target\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# print first 5 rows of X before normalization\n",
        "X[0:5,:]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vYrezGE7Qhc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "bebba6a0-5133-4bee-8e3e-e9d04e0c91c6"
      },
      "source": [
        "# normalize X\n",
        "scaler = Normalizer().fit(X)\n",
        "normalize_X = scaler.transform(X)\n",
        "\n",
        "# print first 5 rows of X after normalization\n",
        "normalize_X[0:5,:]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.80377277, 0.55160877, 0.22064351, 0.0315205 ],\n",
              "       [0.82813287, 0.50702013, 0.23660939, 0.03380134],\n",
              "       [0.80533308, 0.54831188, 0.2227517 , 0.03426949],\n",
              "       [0.80003025, 0.53915082, 0.26087943, 0.03478392],\n",
              "       [0.790965  , 0.5694948 , 0.2214702 , 0.0316386 ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2VSydZR7X6L",
        "colab_type": "text"
      },
      "source": [
        "### Binarization\n",
        "\n",
        "Binarization is a transformation technique for converting a dataset into binary values by setting a cutoff or threshold. All values above the threshold are set to 1, while those\n",
        "below are set to 0. This technique is useful for converting a dataset of probabilities into integer values or in transforming a feature to reflect some categorization. `scikit-learn` implements binarization with the `Binarizer` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtdgv0iM7Vvm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a5d34ce8-2f78-495a-e0b9-63187081d698"
      },
      "source": [
        "# import packages\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "# load dataset\n",
        "data = datasets.load_iris()\n",
        "\n",
        "# separate features and target\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# first 5 rows of X before binarization\n",
        "X[0:5,:]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7VcI6BP7s3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "baf67390-38a6-410e-943b-998270883a95"
      },
      "source": [
        "# binarize X\n",
        "scaler = Binarizer(threshold = 1.5).fit(X)\n",
        "binarize_X = scaler.transform(X)\n",
        "\n",
        "# first 5 rows of X after binarization\n",
        "binarize_X[0:5,:]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 0., 0.],\n",
              "       [1., 1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHSANnhW77uj",
        "colab_type": "text"
      },
      "source": [
        "### Encoding Categorical Variables\n",
        "\n",
        "Most machine learning algorithms do not compute with non-numerical or categorical\n",
        "variables. Hence, encoding categorical variables is the technique for converting non-numerical features with labels into a numerical representation for use in machine learning modeling. `scikit-learn` provides modules for encoding categorical variables\n",
        "including the `LabelEncoder` for encoding labels as integers, `OneHotEncoder` for converting categorical features into a matrix of integers, and `LabelBinarizer` for creating a one-hot encoding of target labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTXIc0VS8Xn5",
        "colab_type": "text"
      },
      "source": [
        "`LabelEncoder` is typically used on the target variable to transform a vector of\n",
        "hashable categories (or labels) into an integer representation by encoding label with values between 0 and the number of categories minus 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2-TOofe7274",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c114692b-2f19-486e-caba-40bcc9203d13"
      },
      "source": [
        "# import packages\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# create dataset\n",
        "data = np.array([[5, 8, \"calabar\"], [9, 3, \"uyo\"],\n",
        "                 [8, 6, \"owerri\"], [0, 5, \"uyo\"], \n",
        "                 [2, 3, \"calabar\"], [0, 8, \"calabar\"],\n",
        "                 [1, 8, \"owerri\"]])\n",
        "data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['5', '8', 'calabar'],\n",
              "       ['9', '3', 'uyo'],\n",
              "       ['8', '6', 'owerri'],\n",
              "       ['0', '5', 'uyo'],\n",
              "       ['2', '3', 'calabar'],\n",
              "       ['0', '8', 'calabar'],\n",
              "       ['1', '8', 'owerri']], dtype='<U21')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbM1kZhL8uCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e56d7506-110a-46ac-a081-ce197574e015"
      },
      "source": [
        "# separate features and target\n",
        "X = data[:,:2]\n",
        "y = data[:, -1]\n",
        "\n",
        "# encode y\n",
        "encoder = LabelEncoder()\n",
        "encode_y = encoder.fit_transform(y)\n",
        "\n",
        "# adjust dataset with encoded targets\n",
        "data[:,-1] = encode_y\n",
        "data"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['5', '8', '0'],\n",
              "       ['9', '3', '2'],\n",
              "       ['8', '6', '1'],\n",
              "       ['0', '5', '2'],\n",
              "       ['2', '3', '0'],\n",
              "       ['0', '8', '0'],\n",
              "       ['1', '8', '1']], dtype='<U21')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WnfBM_19IkW",
        "colab_type": "text"
      },
      "source": [
        "`OneHotEncoder` is used to transform a categorical feature variable in a matrix of integers. This matrix is a sparse matrix with each column corresponding to one possible value of a category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnGYXqlp8-X4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "34460c2a-5ec1-42d1-a31d-ebaca517081e"
      },
      "source": [
        "# import packages\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# create dataset\n",
        "data = np.array([[5, \"efik\", 8, \"calabar\"],\n",
        "                 [9, \"ibibio\", 3, \"uyo\"],\n",
        "                 [8, \"igbo\", 6, \"owerri\"],\n",
        "                 [0, \"ibibio\", 5, \"uyo\"],\n",
        "                 [2, \"efik\", 3, \"calabar\"],\n",
        "                 [0, \"efik\", 8, \"calabar\"],\n",
        "                 [1, \"igbo\", 8, \"owerri\"]])\n",
        "\n",
        "# separate features and target\n",
        "X = data[:, :3]\n",
        "y = data[:, -1]\n",
        "\n",
        "# print the feature or design matrix X\n",
        "X"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['5', 'efik', '8'],\n",
              "       ['9', 'ibibio', '3'],\n",
              "       ['8', 'igbo', '6'],\n",
              "       ['0', 'ibibio', '5'],\n",
              "       ['2', 'efik', '3'],\n",
              "       ['0', 'efik', '8'],\n",
              "       ['1', 'igbo', '8']], dtype='<U21')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONiVxBYQ94IW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "11809a87-bd44-4b76-a4e0-d55557208fc8"
      },
      "source": [
        "# one_hot_encode X\n",
        "one_hot_encoder = OneHotEncoder(handle_unknown = 'ignore')\n",
        "encode_categorical = X[:, 1].reshape(len(X[:, 1]), 1)\n",
        "one_hot_encode_X = one_hot_encoder.fit_transform(encode_categorical)\n",
        "\n",
        "# print one_hot encoded matrix - use todense() to print sparse matrix\n",
        "# or convert to array with toarray()\n",
        "one_hot_encode_X.todense()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [0., 1., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PLO1Qfr-Jwq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "615a8e7f-040e-42c6-dd21-e3f23b01f27b"
      },
      "source": [
        "# remove categorical label\n",
        "X = np.delete(X, 1, axis=1)\n",
        "\n",
        "# append encoded matrix\n",
        "X = np.append(X, one_hot_encode_X.toarray(), axis=1)\n",
        "X"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['5', '8', '1.0', '0.0', '0.0'],\n",
              "       ['9', '3', '0.0', '1.0', '0.0'],\n",
              "       ['8', '6', '0.0', '0.0', '1.0'],\n",
              "       ['0', '5', '0.0', '1.0', '0.0'],\n",
              "       ['2', '3', '1.0', '0.0', '0.0'],\n",
              "       ['0', '8', '1.0', '0.0', '0.0'],\n",
              "       ['1', '8', '0.0', '0.0', '1.0']], dtype='<U32')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OC5ohRg-YL1",
        "colab_type": "text"
      },
      "source": [
        "### Input Missing Data\n",
        "It is often the case that a dataset contains several missing observations. `scikit-learn` implements the `Imputer` module for completing missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVEhV-5y-SNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f10c09ef-4ff6-4d07-b1ff-fb965e17f794"
      },
      "source": [
        "# import packages\n",
        "from sklearn. impute import SimpleImputer\n",
        "\n",
        "# create dataset\n",
        "data = np.array([[5, np.nan, 8], \n",
        "                 [9, 3, 5],\n",
        "                 [8, 6, 4],\n",
        "                 [np.nan, 5, 2],\n",
        "                 [2, 3, 9],\n",
        "                 [np.nan, 8, 7],\n",
        "                 [1, np.nan, 5]])\n",
        "data"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 5., nan,  8.],\n",
              "       [ 9.,  3.,  5.],\n",
              "       [ 8.,  6.,  4.],\n",
              "       [nan,  5.,  2.],\n",
              "       [ 2.,  3.,  9.],\n",
              "       [nan,  8.,  7.],\n",
              "       [ 1., nan,  5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNkTNxyH-sAK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2fcf5af9-852a-4285-a159-3c3d6329237a"
      },
      "source": [
        "# impute missing values - axis=0: impute along columns\n",
        "imputer = SimpleImputer(missing_values=np.nan,\n",
        "                        strategy='mean')\n",
        "imputer.fit_transform(data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5., 5., 8.],\n",
              "       [9., 3., 5.],\n",
              "       [8., 6., 4.],\n",
              "       [5., 5., 2.],\n",
              "       [2., 3., 9.],\n",
              "       [5., 8., 7.],\n",
              "       [1., 5., 5.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itXxaQZi-6tU",
        "colab_type": "text"
      },
      "source": [
        "### Generating Higher-Order Polynomial Features\n",
        "`scikit-learn` has a module called `PolynomialFeatures` for generating a new dataset containing high-order polynomial and interaction features based off the features in the original dataset. For example, if the original dataset has two dimensions $[a, b]$, the\n",
        "second-degree polynomial transformation of the features will result in $[1, a, b, a^2, ab, b^2]$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdGbYZae-y2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0e38c566-bfe7-4312-8eec-fe5ea5077b5e"
      },
      "source": [
        "# import packages\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# create dataset\n",
        "data = np.array([[5, 8], [9, 3], [8, 6], [5, 2],\n",
        "                 [3, 9], [8, 7], [1, 5]])\n",
        "data"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5, 8],\n",
              "       [9, 3],\n",
              "       [8, 6],\n",
              "       [5, 2],\n",
              "       [3, 9],\n",
              "       [8, 7],\n",
              "       [1, 5]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diuP6iJL_VtX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4157d346-6f12-48d7-c2a1-440a40b4549d"
      },
      "source": [
        "# create polynomial features\n",
        "polynomial_features = PolynomialFeatures(2)\n",
        "data = polynomial_features.fit_transform(data)\n",
        "data"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.,  5.,  8., 25., 40., 64.],\n",
              "       [ 1.,  9.,  3., 81., 27.,  9.],\n",
              "       [ 1.,  8.,  6., 64., 48., 36.],\n",
              "       [ 1.,  5.,  2., 25., 10.,  4.],\n",
              "       [ 1.,  3.,  9.,  9., 27., 81.],\n",
              "       [ 1.,  8.,  7., 64., 56., 49.],\n",
              "       [ 1.,  1.,  5.,  1.,  5., 25.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}