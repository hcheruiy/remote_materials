{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow 2.0 and Keras",
      "provenance": [],
      "authorship_tag": "ABX9TyNZxnZRSYV5rUTdw3Crt+EQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hcheruiy/remote_materials/blob/master/TensorFlow_2_0_and_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AE7R1nNAZs5",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow (TF) is a specialized numerical computation library for deep learning.\n",
        "\n",
        "The TF API hierarchy is primarily composed of three API levels, the high-level API, the mid-level API which provides components for building neural network models, and the low-level API. (TODO: image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxmDbcDlA2vt",
        "colab_type": "text"
      },
      "source": [
        "# The Low-Level TensorFlow APIs\n",
        "The low-level API gives the tools for building network graphs from the ground up using mathematical operations. This API level affords the greatest level of flexibility to tweak and tune the model as desired. Moreover, the higher-level APIs implement low-level operations under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6d-yw-rBLyv",
        "colab_type": "text"
      },
      "source": [
        "# The Mid-Level TensorFlow APIs\n",
        "TensorFlow provides a set of reusable packages for simplifying the process involved in creating neural network models. Some examples of these functions include:\n",
        "- __layers__ : `tf.keras.layers`\n",
        "- __Datasets__: `tf.data`\n",
        "- __metrics__: `tf.keras.metrics`\n",
        "- __loss__ : `tf.keras.losses`), and\n",
        "- __FeatureColumns__: `tf.feature_column`\n",
        "\n",
        "## Layers\n",
        "The layers (`tf.keras.layers`) package provides a handy set of functions to simplify the construction of layers in a neural network architecture.\n",
        "\n",
        "## Datasets\n",
        "The Dataset package (`tf.data`) provides a convenient set of high-level functions for creating complex dataset input pipelines. The goal of the Dataset package is to have a fast, flexible, and easy-to-use interface for fetching data from various data sources, performing data transform operations on them before passing them as inputs to the learning model. The Dataset API provides a more efficient means of fetching records\n",
        "from a dataset. The major classes of the Dataset API are:\n",
        "\n",
        "- `TextLineDataset`: used for reading lines from text files.\n",
        "- `TFRecordDataset`: responsible for reading records from `TFRecord` files. A `TFRecord` file is a TensorFlow binary storage format. It is faster and easier to work with data stored as `TFRecord`\n",
        "files as opposed to raw data files. Working with `TFRecord` also\n",
        "makes the data input pipeline more easily aligned for applying vital\n",
        "transformations such as shuffling and returning data in batches.\n",
        "- `FixedLengthRecordDataset`: responsible for reading records of fixed sizes from binary files.\n",
        "\n",
        "## `FeatureColumns`\n",
        "FeatureColumns `tf.feature_column` is a functionality for describing the\n",
        "features of the dataset that will be fed into a high-level Keras or Estimator models for training and validation. FeatureColumns makes it easy to prepare data for modeling by carrying out tasks such as the conversion of categorical features of the dataset into a one-hot\n",
        "encoded vector. The feature_column API is broadly divided into two categories: they are the __categorical__ and __dense columns__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj7vxXd0__wA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "d707f86f-864a-49ef-dbb0-e23888813b00"
      },
      "source": [
        "# A simple TensorFlow Program: a graph to find the roots\n",
        "# of the quadratic expression x^2 + 3x âˆ’ 4 = 0\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "# import tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# Quadratic expression: X**2 + 3x - 4 = 0\n",
        "a = tf.constant(1.0)\n",
        "b = tf.constant(3.0)\n",
        "c = tf.constant(-4.0)\n",
        "\n",
        "print(a, b, c)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "tf.Tensor(1.0, shape=(), dtype=float32) tf.Tensor(3.0, shape=(), dtype=float32) tf.Tensor(-4.0, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yeei1ozNEWAT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f90c0310-b20a-490a-d14b-9c400382d34e"
      },
      "source": [
        "x1 = (-b + tf.math.sqrt(b**2 - (4*a*c))) / 2**a\n",
        "x2 = (-b - tf.math.sqrt(b**2 - (4*a*c))) / 2**a\n",
        "roots = (x1, x2)\n",
        "print(roots)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, <tf.Tensor: shape=(), dtype=float32, numpy=-4.0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUNV8JDkFLKl",
        "colab_type": "text"
      },
      "source": [
        "## Building Efficient Input Pipelines with the Dataset API\n",
        "The Dataset API `tf.data` offers an efficient mechanism for building robust input pipelines for passing data into a TensorFlow program. This section uses the Boston housing dataset to illustrate working with the Dataset API methods for building data input pipelines in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r1geu1NFBaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "8524d08d-7f47-4154-c94d-2c01c9e299d8"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "# import packages\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "\n",
        "# load dataset and split in train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "# construct data input pipelines\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "dataset = dataset.shuffle(buffer_size = 1000)\n",
        "dataset = dataset.batch(5)\n",
        "\n",
        "# retrieve first data batch from dataset\n",
        "for features, labels in dataset:\n",
        "  print('Features:', features)\n",
        "  print('Shape of Features:', features.shape)\n",
        "  print('Labels:', labels)\n",
        "  print('Shape of Labels:', labels.shape)\n",
        "  \n",
        "  break"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "Features: tf.Tensor(\n",
            "[[2.37934e+00 0.00000e+00 1.95800e+01 0.00000e+00 8.71000e-01 6.13000e+00\n",
            "  1.00000e+02 1.41910e+00 5.00000e+00 4.03000e+02 1.47000e+01 1.72910e+02\n",
            "  2.78000e+01]\n",
            " [1.13081e+00 0.00000e+00 8.14000e+00 0.00000e+00 5.38000e-01 5.71300e+00\n",
            "  9.41000e+01 4.23300e+00 4.00000e+00 3.07000e+02 2.10000e+01 3.60170e+02\n",
            "  2.26000e+01]\n",
            " [1.36781e+01 0.00000e+00 1.81000e+01 0.00000e+00 7.40000e-01 5.93500e+00\n",
            "  8.79000e+01 1.82060e+00 2.40000e+01 6.66000e+02 2.02000e+01 6.89500e+01\n",
            "  3.40200e+01]\n",
            " [6.56650e-01 2.00000e+01 3.97000e+00 0.00000e+00 6.47000e-01 6.84200e+00\n",
            "  1.00000e+02 2.01070e+00 5.00000e+00 2.64000e+02 1.30000e+01 3.91930e+02\n",
            "  6.90000e+00]\n",
            " [4.54192e+00 0.00000e+00 1.81000e+01 0.00000e+00 7.70000e-01 6.39800e+00\n",
            "  8.80000e+01 2.51820e+00 2.40000e+01 6.66000e+02 2.02000e+01 3.74560e+02\n",
            "  7.79000e+00]], shape=(5, 13), dtype=float64)\n",
            "Shape of Features: (5, 13)\n",
            "Labels: tf.Tensor([13.8 12.7  8.4 30.1 25. ], shape=(5,), dtype=float64)\n",
            "Shape of Labels: (5,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV6ov2J6FxK7",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression with TensorFlow\n",
        "In this section, we use TensorFlow to implement a linear regression machine learning model. In the following example, we use the Boston house-prices dataset from the Keras\n",
        "dataset package to build a linear regression model with TensorFlow 2.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4kab_WzFqAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "# import packages\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import boston_housing\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# load dataset and split in train and test sets\n",
        "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
        "\n",
        "# standardize the dataset\n",
        "scaler_X_train = StandardScaler().fit(X_train)\n",
        "scaler_X_test = StandardScaler().fit(X_test)\n",
        "X_train = scaler_X_train.transform(X_train)\n",
        "X_test = scaler_X_test.transform(X_test)\n",
        "\n",
        "# reshape y-data to become column vector\n",
        "y_train = np.reshape(y_train, [-1, 1])\n",
        "y_test = np.reshape(y_test, [-1, 1])\n",
        "\n",
        "# build the linear model\n",
        "class LinearRegressionModel(Model):\n",
        "  def __init__(self):\n",
        "    super(LinearRegressionModel, self).__init__()\n",
        "    # initialize weight and bias variables\n",
        "    self.weight = tf.Variable(\n",
        "        initial_value = tf. random.normal(\n",
        "            [13, 1], dtype=tf.float64),\n",
        "            trainable = True\n",
        "    )\n",
        "    self.bias = tf.Variable(initial_value = tf.constant(\n",
        "        1.0, shape=[], dtype=tf.float64),\n",
        "        trainable = True\n",
        "    )\n",
        "    def call(self, inputs):\n",
        "      return tf.add(tf.matmul(inputs, self.weight), self.bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrFVR5IVGvFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LinearRegressionModel()\n",
        "\n",
        "# parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.01\n",
        "\n",
        "# use tf.data to batch and shuffle the dataset\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "(X_train, y_train)).shuffle(len(X_train)).batch(batch_size)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size)\n",
        "loss_object = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_rmse = tf.keras.metrics.RootMeanSquaredError(name='train_rmse')\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_rmse = tf.keras.metrics.RootMeanSquaredError(name='test_rmse')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmGqVmM3G5z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use tf.GradientTape to train the model\n",
        "@tf.function\n",
        "def train_step(inputs, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs)\n",
        "    loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_rmse(labels, predictions)\n",
        "\n",
        "@tf.function\n",
        "def test_step(inputs, labels):\n",
        "  predictions = model(inputs)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "  test_loss(t_loss)\n",
        "  test_rmse(labels, predictions)\n",
        "  \n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "  for train_inputs, train_labels in train_ds:\n",
        "    train_step(train_inputs, train_labels)\n",
        "  for test_inputs, test_labels in test_ds:\n",
        "    test_step(test_inputs, test_labels)\n",
        "    \n",
        "  template = 'Epoch {}, Loss: {}, RMSE: {}, Test Loss: {}, Test RMSE: {}'\n",
        "  \n",
        "  if ((epoch+1) % 100 == 0):\n",
        "    print (template.format(epoch+1,\n",
        "                           train_loss.result(),\n",
        "                           train_rmse.result(),\n",
        "                           test_loss.result(),\n",
        "                           test_rmse.result()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MOwzMocHZAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}